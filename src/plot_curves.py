#!/usr/bin/env python3
"""
FunÃ§Ãµes para criar grÃ¡ficos de ROC Curve e Precision-Recall Curve
a partir dos resultados dos testes de algoritmos de classificaÃ§Ã£o.
"""

import os
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, precision_recall_curve, auc
from plot_curves_multiclass import generate_multiclass_plots
import warnings
warnings.filterwarnings('ignore')

# Adicionar path do projeto
sys.path.append('/Users/i583975/git/tcc')

# ConfiguraÃ§Ã£o do matplotlib para melhor qualidade
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 10
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['legend.fontsize'] = 10

# ConfiguraÃ§Ã£o do seaborn
sns.set_style("whitegrid")
sns.set_palette("husl")

# Paleta de cores padronizada para todos os modelos
STANDARD_COLORS = {
    'decision_tree': '#1f77b4',           # Azul
    'random_forest': '#ff7f0e',           # Laranja
    'gradient_boosting': '#2ca02c',       # Verde
    'histogram_gradient_boosting': '#d62728',  # Vermelho
    'k_nearest_neighbors': '#9467bd',     # Roxo
    'multi_layer_perceptron': '#8c564b',  # Marrom
    'support_vector_classifier': '#e377c2',  # Rosa
    'catboost': '#17becf',                # Ciano
}


def create_plots_directory(experiment_dir):
    """Cria diretÃ³rio curves dentro do diretÃ³rio do experimento para salvar os grÃ¡ficos"""
    curves_dir = os.path.join(experiment_dir, "curves")
    os.makedirs(curves_dir, exist_ok=True)
    
    return curves_dir


def load_saved_predictions(results_dir="./results"):
    """
    Carrega prediÃ§Ãµes salvas de todos os modelos testados
    
    Args:
        results_dir (str): DiretÃ³rio para procurar resultados
        
    Returns:
        tuple: (models_data, experiment_dir) - DicionÃ¡rio com prediÃ§Ãµes e caminho do experimento
    """
    models_data = {}
    experiment_dir = None
    
    # Lista de modelos padronizada (sem variaÃ§Ãµes)
    model_names = [
        'decision_tree', 'random_forest', 'gradient_boosting', 
        'histogram_gradient_boosting', 'k_nearest_neighbors', 'multi_layer_perceptron', 
        'support_vector_classifier', 'catboost'
    ]
    
    print(f"Procurando resultados em: {results_dir}")
    
    if not os.path.exists(results_dir):
        print(f"DiretÃ³rio {results_dir} nÃ£o existe")
        return models_data, experiment_dir
    
    # Listar subdiretÃ³rios de experimentos (formato: YYYYMMDD_HHMMSS_ana_default_*)
    experiment_dirs = []
    for item in os.listdir(results_dir):
        item_path = os.path.join(results_dir, item)
        if os.path.isdir(item_path):
            # Verificar se segue o padrÃ£o de timestamp de experimento
            if len(item) > 15 and item[8] == '_' and item[15] == '_' and 'ana_default' in item:
                experiment_dirs.append(item)
    
    if not experiment_dirs:
        print(f"Nenhum experimento encontrado em {results_dir}")
        print(f"DiretÃ³rios disponÃ­veis: {os.listdir(results_dir)}")
        return models_data, experiment_dir
    
    # Ordenar diretÃ³rios por timestamp (mais recente primeiro)
    experiment_dirs.sort(reverse=True)
    
    # Mostrar opÃ§Ãµes para o usuÃ¡rio
    print("\nDiretÃ³rios de experimentos disponÃ­veis:")
    for i, dir_name in enumerate(experiment_dirs, 1):
        # Extrair informaÃ§Ãµes do nome do diretÃ³rio para melhor visualizaÃ§Ã£o
        parts = dir_name.split('_')
        if len(parts) >= 4:
            date_part = parts[0]  # YYYYMMDD
            time_part = parts[1]  # HHMMSS
            classification_type = parts[-1]  # binary/multiclass
            formatted_date = f"{date_part[:4]}-{date_part[4:6]}-{date_part[6:8]}"
            formatted_time = f"{time_part[:2]}:{time_part[2:4]}:{time_part[4:6]}"
            print(f"{i}. {dir_name}")
            print(f"   Data: {formatted_date} {formatted_time} | Tipo: {classification_type}")
        else:
            print(f"{i}. {dir_name}")
    
    # Solicitar seleÃ§Ã£o do usuÃ¡rio
    print(f"\n[Enter] = usar o mais recente ({experiment_dirs[0]})")
    choice = input("Selecione o nÃºmero do experimento (ou pressione Enter): ").strip()
    
    if choice == "":
        # Usar o experimento mais recente (primeiro da lista ordenada)
        selected_experiment = experiment_dirs[0]
        print(f"Usando experimento mais recente: {selected_experiment}")
    else:
        try:
            choice_num = int(choice)
            if 1 <= choice_num <= len(experiment_dirs):
                selected_experiment = experiment_dirs[choice_num - 1]
                print(f"Usando experimento selecionado: {selected_experiment}")
            else:
                print(f"NÃºmero invÃ¡lido. Usando experimento mais recente: {experiment_dirs[0]}")
                selected_experiment = experiment_dirs[0]
        except ValueError:
            print(f"Entrada invÃ¡lida. Usando experimento mais recente: {experiment_dirs[0]}")
            selected_experiment = experiment_dirs[0]
    
    current_results_dir = os.path.join(results_dir, selected_experiment)
    experiment_dir = current_results_dir  # Armazenar o caminho do experimento
    
    # Procurar modelos com nomes padronizados e variaÃ§Ãµes (para compatibilidade)
    model_variations = {
        'decision_tree': ['decision_tree'],
        'random_forest': ['random_forest'],
        'gradient_boosting': ['gradient_boosting'],
        'histogram_gradient_boosting': ['histogram_gradient_boosting'],
        'k_nearest_neighbors': ['k_nearest_neighbors', 'k-nearest_neighbors'],
        'multi_layer_perceptron': ['multi_layer_perceptron', 'multi-layer_perceptron'],
        'support_vector_classifier': ['support_vector_classifier'],
        'catboost': ['catboost']
    }
    
    for standard_name, variations in model_variations.items():
        for model_name in variations:
            model_dir = os.path.join(current_results_dir, model_name)
            print(f"Verificando {model_name} em {model_dir}...")
            
            if os.path.exists(model_dir):
                # Procurar por arquivos metrics.json (com diferentes prefixos)
                metrics_files = [
                    os.path.join(model_dir, 'metrics.json'),          # PadrÃ£o sem prefixo
                    os.path.join(model_dir, 'default_metrics.json'),  # Com prefixo default_
                ]
                
                metrics_file = None
                for mf in metrics_files:
                    if os.path.exists(mf):
                        metrics_file = mf
                        break
                
                if metrics_file:
                    print(f"   Carregando prediÃ§Ãµes de {standard_name}")
                    print(f"   Arquivo: {os.path.basename(metrics_file)}")
                    
                    try:
                        with open(metrics_file, 'r') as f:
                            data = json.load(f)
                        
                        # Extrair prediÃ§Ãµes
                        if 'test_predictions' in data:
                            predictions = data['test_predictions']
                            
                            if predictions and all(key in predictions for key in ['y_true', 'y_pred_proba']):
                                models_data[standard_name] = {
                                    'predictions': predictions,
                                    'file_path': metrics_file
                                }
                                print(f"   âœ“ PrediÃ§Ãµes carregadas: {len(predictions['y_true'])} amostras")
                                break  # Encontrou o modelo, nÃ£o precisa procurar outras variaÃ§Ãµes
                            else:
                                print(f"   âš ï¸  {standard_name}: PrediÃ§Ãµes nÃ£o encontradas no arquivo")
                        else:
                            print(f"   âš ï¸  {standard_name}: Campo 'test_predictions' nÃ£o encontrado")
                            
                    except Exception as e:
                        print(f"   âŒ Erro ao carregar {standard_name}: {e}")
                else:
                    print(f"   âš ï¸  Nenhum arquivo de mÃ©tricas encontrado para {model_name}")
                    print(f"       Procurou por: metrics.json, default_metrics.json")
            else:
                print(f"   âš ï¸  DiretÃ³rio nÃ£o existe: {model_dir}")
    
    return models_data, experiment_dir


def detect_classification_type(y_true, y_pred_proba):
    """
    Detecta automaticamente se Ã© classificaÃ§Ã£o binÃ¡ria ou multiclasse
    
    Args:
        y_true: Array com labels verdadeiros
        y_pred_proba: Array com probabilidades preditas
    
    Returns:
        tuple: (classification_type, n_classes, class_names)
    """
    n_unique_classes = len(np.unique(y_true))
    
    # Verificar se y_pred_proba Ã© 1D (binÃ¡rio) ou 2D (multiclasse)
    if len(np.array(y_pred_proba).shape) == 1:
        # BinÃ¡rio com apenas probabilidades da classe positiva
        return 'binary', 2, ['Class 0', 'Class 1']
    else:
        y_pred_proba_array = np.array(y_pred_proba)
        n_prob_classes = y_pred_proba_array.shape[1] if len(y_pred_proba_array.shape) > 1 else 1
        
        if n_unique_classes == 2 and n_prob_classes == 2:
            return 'binary', 2, ['Class 0', 'Class 1']
        elif n_unique_classes > 2 and n_prob_classes > 2:
            class_names = [f'Class {i}' for i in range(n_unique_classes)]
            return 'multiclass', n_unique_classes, class_names
        else:
            # Fallback para binÃ¡rio
            return 'binary', 2, ['Class 0', 'Class 1']


def plot_roc_curve(model_results, save_path=None):
    """
    Cria grÃ¡fico de ROC Curve para todos os modelos usando prediÃ§Ãµes salvas
    
    Args:
        model_results (dict): Resultados dos modelos com prediÃ§Ãµes
        save_path (str): Caminho para salver o grÃ¡fico
    """
    plt.figure(figsize=(12, 9))
    
    models_plotted = 0
    models_with_errors = []
    classification_type = None
    
    # Ordenar modelos para plotagem consistente
    sorted_models = sorted(model_results.items())
    
    for model_name, data in sorted_models:
        print(f"Processando {model_name}...")
        
        try:
            predictions = data['predictions']
            y_true = np.array(predictions['y_true'])
            y_pred_proba = np.array(predictions['y_pred_proba'])
            
            # Detectar tipo de classificaÃ§Ã£o (apenas uma vez)
            if classification_type is None:
                classification_type, n_classes, class_names = detect_classification_type(y_true, y_pred_proba)
                print(f"  Tipo de classificaÃ§Ã£o detectado: {classification_type} ({n_classes} classes)")
            
            model_display_name = model_name.replace('_', ' ').title()
            color = STANDARD_COLORS.get(model_name, f'C{models_plotted}')
            
            if classification_type == 'binary':
                # ClassificaÃ§Ã£o binÃ¡ria
                if len(y_pred_proba.shape) == 2:
                    # Se temos probabilidades para ambas as classes, usar a classe positiva
                    y_pred_proba_pos = y_pred_proba[:, 1]
                else:
                    # Se temos apenas probabilidades da classe positiva
                    y_pred_proba_pos = y_pred_proba
                
                fpr, tpr, _ = roc_curve(y_true, y_pred_proba_pos)
                roc_auc = auc(fpr, tpr)
                
                plt.plot(fpr, tpr, color=color, lw=3, 
                        label=f'{model_display_name} (AUC = {roc_auc:.3f})')
                print(f"  {model_name}: AUC = {roc_auc:.3f}")

            models_plotted += 1
            
        except Exception as e:
            models_with_errors.append(model_name)
            print(f"  âŒ {model_name}: Erro ao processar prediÃ§Ãµes - {e}")
    
    # Linha diagonal (classificador aleatÃ³rio)
    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', alpha=0.8, 
             label='Random Classifier (AUC = 0.500)')
    
    # ConfiguraÃ§Ãµes do grÃ¡fico
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (FPR)', fontsize=22)
    plt.ylabel('True Positive Rate (TPR)', fontsize=22)
    
    plt.title('ROC Curves (Binary Classification)', fontsize=24)
    
    plt.legend(loc="lower right", fontsize=18, framealpha=0.9)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path + '_binary.png', dpi=300, bbox_inches='tight')
        plt.savefig(save_path + '_binary.pdf', bbox_inches='tight')
        print(f"ROC Curve salva em: " + save_path + '_binary')
    
    # RelatÃ³rio final
    print(f"\nRelatÃ³rio ROC:")
    print(f"  Tipo de classificaÃ§Ã£o: {classification_type}")
    print(f"  Modelos plotados: {models_plotted}")
    if models_with_errors:
        print(f"  Modelos com erro: {', '.join(models_with_errors)}")


def plot_precision_recall_curve(model_results, save_path=None):
    """
    Cria grÃ¡fico de Precision-Recall Curve para todos os modelos usando prediÃ§Ãµes salvas
    
    Args:
        model_results (dict): Resultados dos modelos com prediÃ§Ãµes
        save_path (str): Caminho para salvar o grÃ¡fico
    """
    plt.figure(figsize=(12, 9))
    
    models_plotted = 0
    models_with_errors = []
    pos_rate = None
    classification_type = None
    
    # Ordenar modelos para plotagem consistente
    sorted_models = sorted(model_results.items())
    
    for model_name, data in sorted_models:
        print(f"Processando {model_name}...")
        
        try:
            predictions = data['predictions']
            y_true = np.array(predictions['y_true'])
            y_pred_proba = np.array(predictions['y_pred_proba'])
            
            # Detectar tipo de classificaÃ§Ã£o (apenas uma vez)
            if classification_type is None:
                classification_type, n_classes, class_names = detect_classification_type(y_true, y_pred_proba)
                print(f"  Tipo de classificaÃ§Ã£o detectado: {classification_type} ({n_classes} classes)")
            
            model_display_name = model_name.replace('_', ' ').title()
            color = STANDARD_COLORS.get(model_name, f'C{models_plotted}')
            
            if classification_type == 'binary':
                # ClassificaÃ§Ã£o binÃ¡ria
                if pos_rate is None:
                    pos_rate = np.mean(y_true)
                
                if len(y_pred_proba.shape) == 2:
                    # Se temos probabilidades para ambas as classes, usar a classe positiva
                    y_pred_proba_pos = y_pred_proba[:, 1]
                else:
                    # Se temos apenas probabilidades da classe positiva
                    y_pred_proba_pos = y_pred_proba
                
                precision, recall, _ = precision_recall_curve(y_true, y_pred_proba_pos)
                pr_auc = auc(recall, precision)
                
                plt.plot(recall, precision, color=color, lw=3, 
                        label=f'{model_display_name} (AUC = {pr_auc:.3f})')
                print(f"  {model_name}: PR AUC = {pr_auc:.3f}")
            
            models_plotted += 1
            
        except Exception as e:
            models_with_errors.append(model_name)
            print(f"  âŒ {model_name}: Erro ao processar prediÃ§Ãµes - {e}")
    
    # Linha base (classificador aleatÃ³rio)
    if pos_rate is not None:
        plt.axhline(y=pos_rate, color='gray', lw=2, linestyle='--', alpha=0.8,
                    label=f'Random Classifier (AUC = {pos_rate:.3f})')
    
    # ConfiguraÃ§Ãµes do grÃ¡fico
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall', fontsize=22)
    plt.ylabel('Precision', fontsize=22)
    
    plt.title('Precision-Recall Curves (Binary Classification)', fontsize=24)
    
    plt.legend(loc="lower left", fontsize=18, framealpha=0.9)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path + '_binary.png', dpi=300, bbox_inches='tight')
        plt.savefig(save_path + '_binary.pdf', bbox_inches='tight')
        print(f"Precision-Recall Curve salva em: {save_path}{suffix}")
        
    # RelatÃ³rio final
    print(f"\nRelatÃ³rio Precision-Recall:")
    print(f"  Modelos plotados: {models_plotted}")
    if models_with_errors:
        print(f"  Modelos com erro: {', '.join(models_with_errors)}")


def generate_all_plots():
    """
    FunÃ§Ã£o principal para gerar todos os grÃ¡ficos usando prediÃ§Ãµes salvas
    """
    print("Gerando grÃ¡ficos de performance dos modelos usando prediÃ§Ãµes salvas...")
    print("="*70)
    
    print("Carregando prediÃ§Ãµes salvas dos modelos...")
    model_results, experiment_dir = load_saved_predictions()
    
    if not model_results:
        print("âŒ Nenhuma prediÃ§Ã£o encontrada!")
        print("Execute primeiro os modelos usando main.py para gerar as prediÃ§Ãµes.")
        return
    
    if not experiment_dir:
        print("âŒ Nenhum diretÃ³rio de experimento encontrado!")
        return
    
    # Detectar tipo de classificaÃ§Ã£o
    first_model = next(iter(model_results.values()))
    predictions = first_model['predictions']
    y_true = np.array(predictions['y_true'])
    y_pred_proba = np.array(predictions['y_pred_proba'])
    classification_type, n_classes, class_names = detect_classification_type(y_true, y_pred_proba)
    
    # Criar estrutura de diretÃ³rios
    curves_dir = os.path.join(experiment_dir, "curves")
    os.makedirs(curves_dir, exist_ok=True)
    
    print(f"âœ… PrediÃ§Ãµes encontradas para {len(model_results)} modelos: {list(model_results.keys())}")
    print(f"ðŸ“Š Tipo de classificaÃ§Ã£o: {classification_type} ({n_classes} classes)")
    
    if classification_type == 'multiclass' and n_classes > 2:
        print(f"\nðŸ“ˆ AnÃ¡lise multiclasse detectada ({n_classes} classes)")
        generate_multiclass_plots(model_results, class_names, curves_dir)

    else:
        print(f"\nðŸ“ˆ AnÃ¡lise binÃ¡ria detectada")
        print("\nGerando ROC Curves...")
        roc_save_path = os.path.join(curves_dir, "roc_comparison")
        plot_roc_curve(model_results, save_path=roc_save_path)
        
        print("\nGerando PR Curves...")
        pr_save_path = os.path.join(curves_dir, "pr_comparison")
        plot_precision_recall_curve(model_results, save_path=pr_save_path)
    
    print(f"\nðŸŽ‰ Todos os grÃ¡ficos gerados com sucesso!")

if __name__ == "__main__":
    print("Executando geraÃ§Ã£o de grÃ¡ficos...")
    generate_all_plots()
