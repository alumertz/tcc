#!/usr/bin/env python3
"""
FunÃ§Ãµes para criar grÃ¡ficos de ROC Curve e Precision-Recall Curve
a partir dos resultados dos testes de algoritmos de classificaÃ§Ã£o.
"""

import os
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, precision_recall_curve, auc
from plot_curves_multiclass import generate_multiclass_plots
import warnings
warnings.filterwarnings('ignore')

# Adicionar path do projeto
sys.path.append('/Users/i583975/git/tcc')

# ConfiguraÃ§Ã£o do matplotlib para melhor qualidade
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 10
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['legend.fontsize'] = 10

# ConfiguraÃ§Ã£o do seaborn
sns.set_style("whitegrid")
sns.set_palette("husl")

# Paleta de cores padronizada para todos os modelos
STANDARD_COLORS = {
    'decision_tree': '#1f77b4',           # Azul
    'random_forest': '#ff7f0e',           # Laranja
    'gradient_boosting': '#2ca02c',       # Verde
    'histogram_gradient_boosting': '#d62728',  # Vermelho
    'k_nearest_neighbors': '#9467bd',     # Roxo
    'multi_layer_perceptron': '#8c564b',  # Marrom
    'support_vector_classifier': '#e377c2',  # Rosa
    'catboost': '#17becf',                # Ciano
}


def create_plots_directory(experiment_dir):
    """Cria diretÃ³rio curves dentro do diretÃ³rio do experimento para salvar os grÃ¡ficos"""
    curves_dir = os.path.join(experiment_dir, "curves")
    os.makedirs(curves_dir, exist_ok=True)
    
    return curves_dir


def load_saved_predictions(results_dir="./results"):
    """
    Carrega prediÃ§Ãµes salvas de todos os modelos testados (modo default ou optimized)
    
    Args:
        results_dir (str): DiretÃ³rio para procurar resultados
        
    Returns:
        tuple: (models_data, experiment_dir, mode) - DicionÃ¡rio com prediÃ§Ãµes, caminho do experimento e modo (default/optimized)
    """
    models_data = {}
    experiment_dir = None
    detected_mode = None
    
    # Lista de modelos padronizada (sem variaÃ§Ãµes)
    model_names = [
        'decision_tree', 'random_forest', 'gradient_boosting', 
        'histogram_gradient_boosting', 'k_nearest_neighbors', 'multi_layer_perceptron', 
        'support_vector_classifier', 'catboost', 'svc'
    ]
    
    print(f"Procurando resultados em: {results_dir}")
    
    if not os.path.exists(results_dir):
        print(f"DiretÃ³rio {results_dir} nÃ£o existe")
        return models_data, experiment_dir, detected_mode
    
    # Listar subdiretÃ³rios de experimentos (formato: YYYYMMDD_HHMMSS_ana_default_* ou _optimized_*)
    experiment_dirs = []
    for item in os.listdir(results_dir):
        item_path = os.path.join(results_dir, item)
        if os.path.isdir(item_path):
            # Verificar se segue o padrÃ£o de timestamp de experimento
            if len(item) > 15 and item[8] == '_' and item[15] == '_' and ('binary' in item or 'multiclass' in item):
                experiment_dirs.append(item)
    
    if not experiment_dirs:
        print(f"Nenhum experimento encontrado em {results_dir}")
        print(f"DiretÃ³rios disponÃ­veis: {os.listdir(results_dir)}")
        return models_data, experiment_dir, detected_mode
    
    # Ordenar diretÃ³rios por timestamp (mais recente primeiro)
    experiment_dirs.sort(reverse=True)
    
    # Mostrar opÃ§Ãµes para o usuÃ¡rio
    print("\nDiretÃ³rios de experimentos disponÃ­veis:")
    for i, dir_name in enumerate(experiment_dirs, 1):
        # Extrair informaÃ§Ãµes do nome do diretÃ³rio para melhor visualizaÃ§Ã£o
        parts = dir_name.split('_')
        if len(parts) >= 10:
            date_part = parts[0]  # YYYYMMDD
            time_part = parts[1]  # HHMMSS
            mode_part = parts[3]  # default or optimized
            classification_type = parts[-1]  # binary/multiclass
            formatted_date = f"{date_part[:4]}-{date_part[4:6]}-{date_part[6:8]}"
            formatted_time = f"{time_part[:2]}:{time_part[2:4]}:{time_part[4:6]}"
            print(f"{i}. {dir_name}")
            print(f"   Data: {formatted_date} {formatted_time} | Modo: {mode_part.upper()} | Tipo: {classification_type}")
        else:
            print(f"{i}. {dir_name}")
    
    # Solicitar seleÃ§Ã£o do usuÃ¡rio
    print(f"\n[Enter] = usar o mais recente ({experiment_dirs[0]})")
    choice = input("Selecione o nÃºmero do experimento (ou pressione Enter): ").strip()
    
    if choice == "":
        selected_experiment = experiment_dirs[0]
        print(f"Usando experimento mais recente: {selected_experiment}")
    else:
        try:
            choice_num = int(choice)
            if 1 <= choice_num <= len(experiment_dirs):
                selected_experiment = experiment_dirs[choice_num - 1]
                print(f"Usando experimento selecionado: {selected_experiment}")
            else:
                print(f"NÃºmero invÃ¡lido. Usando experimento mais recente: {experiment_dirs[0]}")
                selected_experiment = experiment_dirs[0]
        except ValueError:
            print(f"Entrada invÃ¡lida. Usando experimento mais recente: {experiment_dirs[0]}")
            selected_experiment = experiment_dirs[0]
    
    current_results_dir = os.path.join(results_dir, selected_experiment)
    experiment_dir = current_results_dir
    
    # Detectar modo (default ou optimized)
    if 'default' in selected_experiment:
        detected_mode = 'default'
    elif 'optimized' in selected_experiment:
        detected_mode = 'optimized'
    else:
        detected_mode = 'unknown'
    
    print(f"\nModo detectado: {detected_mode.upper()}")
    
    # Procurar modelos com nomes padronizados e variaÃ§Ãµes
    model_variations = {
        'decision_tree': ['decision_tree'],
        'random_forest': ['random_forest'],
        'gradient_boosting': ['gradient_boosting'],
        'histogram_gradient_boosting': ['histogram_gradient_boosting'],
        'k_nearest_neighbors': ['k_nearest_neighbors', 'k-nearest_neighbors'],
        'multi_layer_perceptron': ['multi_layer_perceptron', 'multi-layer_perceptron'],
        'support_vector_classifier': ['support_vector_classifier', 'svc'],
        'catboost': ['catboost']
    }
    
    if detected_mode == 'default':
        # Modo default: procurar em pastas de modelos por default_metrics.json
        for standard_name, variations in model_variations.items():
            for model_name in variations:
                model_dir = os.path.join(current_results_dir, model_name)
                
                if os.path.exists(model_dir):
                    metrics_files = [
                        os.path.join(model_dir, 'default_metrics.json'),
                        os.path.join(model_dir, 'metrics.json'),
                    ]
                    
                    for metrics_file in metrics_files:
                        if os.path.exists(metrics_file):
                            print(f"Carregando {standard_name} de {os.path.basename(metrics_file)}...")
                            try:
                                with open(metrics_file, 'r') as f:
                                    data = json.load(f)
                                
                                if 'test_predictions' in data:
                                    predictions = data['test_predictions']
                                    if predictions and 'y_true' in predictions and 'y_pred_proba' in predictions:
                                        models_data[standard_name] = {
                                            'predictions': predictions,
                                            'file_path': metrics_file
                                        }
                                        print(f"  âœ“ {standard_name}: {len(predictions['y_true'])} amostras")
                                        break
                                    else:
                                        print(f"  âš ï¸  {standard_name}: test_predictions incompleto")
                                else:
                                    print(f"  âš ï¸  {standard_name}: sem test_predictions")
                            except Exception as e:
                                print(f"  âŒ {standard_name}: {e}")
                            
                            if standard_name in models_data:
                                break
    
    elif detected_mode == 'optimized':
        # Modo optimized: procurar nested_cv_*.json em modelo folders (novo) ou no root (antigo)
        # EstratÃ©gia:
        # 1. Procurar em pastas de modelos por nested_cv_*.json (novo formato com modelo folder)
        # 2. Se nÃ£o encontrado, procurar nested_cv_*.json no root do experimento (formato antigo)
        
        for standard_name, variations in model_variations.items():
            found = False
            
            # Tenta procurar em pasta de modelo (novo formato)
            for model_name in variations:
                model_dir = os.path.join(current_results_dir, model_name)
                if os.path.exists(model_dir):
                    # Procurar por metrics.json nesta pasta (novo formato otimizado)
                    metrics_file = os.path.join(model_dir, 'metrics.json')
                    if os.path.exists(metrics_file):
                        print(f"Carregando {standard_name} de metrics.json...")
                        try:
                            with open(metrics_file, 'r') as f:
                                data = json.load(f)
                            
                            # Tentar carregar test_predictions do novo formato
                            if 'test_predictions' in data and data['test_predictions']:
                                predictions = data['test_predictions']
                                if 'y_true' in predictions and 'y_pred_proba' in predictions:
                                    models_data[standard_name] = {
                                        'predictions': predictions,
                                        'file_path': metrics_file,
                                        'aggregated_metrics': data.get('aggregated_metrics', {})
                                    }
                                    print(f"  âœ“ {standard_name}: {len(predictions['y_true'])} amostras (test_predictions)")
                                    found = True
                                    break
                            # Se nÃ£o tem test_predictions, usar agregadas (fallback - sem curve real)
                            elif 'aggregated_metrics' in data:
                                agg = data['aggregated_metrics']
                                print(f"  âš ï¸  {standard_name}: usando mÃ©tricas agregadas (sem test_predictions)")
                                models_data[standard_name] = {
                                    'predictions': None,
                                    'file_path': metrics_file,
                                    'aggregated_metrics': agg
                                }
                                found = True
                                break
                        except Exception as e:
                            print(f"  âŒ Erro lendo metrics.json: {e}")
                
                if found:
                    break
            
            # Se nÃ£o encontrou em pasta de modelo, procurar no root (formato antigo)
            if not found:
                try:
                    for file in os.listdir(current_results_dir):
                        if file.startswith('nested_cv_') and standard_name in file.lower() and file.endswith('.json'):
                            nested_cv_file = os.path.join(current_results_dir, file)
                            print(f"Carregando {standard_name} de {file} (root)...")
                            try:
                                with open(nested_cv_file, 'r') as f:
                                    data = json.load(f)
                                
                                if 'test_predictions' in data and data['test_predictions']:
                                    predictions = data['test_predictions']
                                    if 'y_true' in predictions and 'y_pred_proba' in predictions:
                                        models_data[standard_name] = {
                                            'predictions': predictions,
                                            'file_path': nested_cv_file,
                                            'aggregated_metrics': data.get('aggregated_metrics', {})
                                        }
                                        print(f"  âœ“ {standard_name}: {len(predictions['y_true'])} amostras")
                                        found = True
                                        break
                                elif 'aggregated_metrics' in data:
                                    agg = data['aggregated_metrics']
                                    print(f"  âš ï¸  {standard_name}: usando mÃ©tricas agregadas (sem test_predictions)")
                                    models_data[standard_name] = {
                                        'predictions': None,
                                        'file_path': nested_cv_file,
                                        'aggregated_metrics': agg
                                    }
                                    found = True
                                    break
                            except Exception as e:
                                print(f"  âŒ Erro lendo {file}: {e}")
                            
                            if found:
                                break
                except Exception:
                    pass
    
    return models_data, experiment_dir, detected_mode


def detect_classification_type(y_true, y_pred_proba):
    """
    Detecta automaticamente se Ã© classificaÃ§Ã£o binÃ¡ria ou multiclasse
    
    Args:
        y_true: Array com labels verdadeiros
        y_pred_proba: Array com probabilidades preditas
    
    Returns:
        tuple: (classification_type, n_classes, class_names)
    """
    n_unique_classes = len(np.unique(y_true))
    
    # Verificar se y_pred_proba Ã© 1D (binÃ¡rio) ou 2D (multiclasse)
    if len(np.array(y_pred_proba).shape) == 1:
        # BinÃ¡rio com apenas probabilidades da classe positiva
        return 'binary', 2, ['Class 0', 'Class 1']
    else:
        y_pred_proba_array = np.array(y_pred_proba)
        n_prob_classes = y_pred_proba_array.shape[1] if len(y_pred_proba_array.shape) > 1 else 1
        
        if n_unique_classes == 2 and n_prob_classes == 2:
            return 'binary', 2, ['Class 0', 'Class 1']
        elif n_unique_classes > 2 and n_prob_classes > 2:
            class_names = [f'Class {i}' for i in range(n_unique_classes)]
            return 'multiclass', n_unique_classes, class_names
        else:
            # Fallback para binÃ¡rio
            return 'binary', 2, ['Class 0', 'Class 1']


def plot_roc_curve(model_results, save_path=None):
    """
    Cria grÃ¡fico de ROC Curve para todos os modelos usando prediÃ§Ãµes salvas
    
    Args:
        model_results (dict): Resultados dos modelos com prediÃ§Ãµes
        save_path (str): Caminho para salver o grÃ¡fico
    """
    plt.figure(figsize=(12, 9))
    
    models_plotted = 0
    models_with_errors = []
    classification_type = None
    
    # Ordenar modelos para plotagem consistente
    sorted_models = sorted(model_results.items())
    
    for model_name, data in sorted_models:
        print(f"Processando {model_name}...")
        
        try:
            predictions = data['predictions']
            y_true = np.array(predictions['y_true'])
            y_pred_proba = np.array(predictions['y_pred_proba'])
            
            # Detectar tipo de classificaÃ§Ã£o (apenas uma vez)
            if classification_type is None:
                classification_type, n_classes, class_names = detect_classification_type(y_true, y_pred_proba)
                print(f"  Tipo de classificaÃ§Ã£o detectado: {classification_type} ({n_classes} classes)")
            
            model_display_name = model_name.replace('_', ' ').title()
            color = STANDARD_COLORS.get(model_name, f'C{models_plotted}')
            
            if classification_type == 'binary':
                # ClassificaÃ§Ã£o binÃ¡ria
                if len(y_pred_proba.shape) == 2:
                    # Se temos probabilidades para ambas as classes, usar a classe positiva
                    y_pred_proba_pos = y_pred_proba[:, 1]
                else:
                    # Se temos apenas probabilidades da classe positiva
                    y_pred_proba_pos = y_pred_proba
                
                fpr, tpr, _ = roc_curve(y_true, y_pred_proba_pos)
                roc_auc = auc(fpr, tpr)
                
                plt.plot(fpr, tpr, color=color, lw=3, 
                        label=f'{model_display_name} (AUC = {roc_auc:.3f})')
                print(f"  {model_name}: AUC = {roc_auc:.3f}")

            models_plotted += 1
            
        except Exception as e:
            models_with_errors.append(model_name)
            print(f"  âŒ {model_name}: Erro ao processar prediÃ§Ãµes - {e}")
    
    # Linha diagonal (classificador aleatÃ³rio)
    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', alpha=0.8, 
             label='Random Classifier (AUC = 0.500)')
    
    # ConfiguraÃ§Ãµes do grÃ¡fico
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (FPR)', fontsize=22)
    plt.ylabel('True Positive Rate (TPR)', fontsize=22)
    
    plt.title('ROC Curves (Binary Classification)', fontsize=24)
    
    plt.legend(loc="lower right", fontsize=18, framealpha=0.9)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path + '_binary.png', dpi=300, bbox_inches='tight')
        plt.savefig(save_path + '_binary.pdf', bbox_inches='tight')
        print(f"ROC Curve salva em: " + save_path + '_binary')
    
    # RelatÃ³rio final
    print(f"\nRelatÃ³rio ROC:")
    print(f"  Tipo de classificaÃ§Ã£o: {classification_type}")
    print(f"  Modelos plotados: {models_plotted}")
    if models_with_errors:
        print(f"  Modelos com erro: {', '.join(models_with_errors)}")


def plot_precision_recall_curve(model_results, save_path=None):
    """
    Cria grÃ¡fico de Precision-Recall Curve para todos os modelos usando prediÃ§Ãµes salvas
    
    Args:
        model_results (dict): Resultados dos modelos com prediÃ§Ãµes
        save_path (str): Caminho para salvar o grÃ¡fico
    """
    plt.figure(figsize=(12, 9))
    
    models_plotted = 0
    models_with_errors = []
    pos_rate = None
    classification_type = None
    
    # Ordenar modelos para plotagem consistente
    sorted_models = sorted(model_results.items())
    
    for model_name, data in sorted_models:
        print(f"Processando {model_name}...")
        
        try:
            predictions = data['predictions']
            y_true = np.array(predictions['y_true'])
            y_pred_proba = np.array(predictions['y_pred_proba'])
            
            # Detectar tipo de classificaÃ§Ã£o (apenas uma vez)
            if classification_type is None:
                classification_type, n_classes, class_names = detect_classification_type(y_true, y_pred_proba)
                print(f"  Tipo de classificaÃ§Ã£o detectado: {classification_type} ({n_classes} classes)")
            
            model_display_name = model_name.replace('_', ' ').title()
            color = STANDARD_COLORS.get(model_name, f'C{models_plotted}')
            
            if classification_type == 'binary':
                # ClassificaÃ§Ã£o binÃ¡ria
                if pos_rate is None:
                    pos_rate = np.mean(y_true)
                
                if len(y_pred_proba.shape) == 2:
                    # Se temos probabilidades para ambas as classes, usar a classe positiva
                    y_pred_proba_pos = y_pred_proba[:, 1]
                else:
                    # Se temos apenas probabilidades da classe positiva
                    y_pred_proba_pos = y_pred_proba
                
                precision, recall, _ = precision_recall_curve(y_true, y_pred_proba_pos)
                pr_auc = auc(recall, precision)
                
                plt.plot(recall, precision, color=color, lw=3, 
                        label=f'{model_display_name} (AUC = {pr_auc:.3f})')
                print(f"  {model_name}: PR AUC = {pr_auc:.3f}")
            
            models_plotted += 1
            
        except Exception as e:
            models_with_errors.append(model_name)
            print(f"  âŒ {model_name}: Erro ao processar prediÃ§Ãµes - {e}")
    
    # Linha base (classificador aleatÃ³rio)
    if pos_rate is not None:
        plt.axhline(y=pos_rate, color='gray', lw=2, linestyle='--', alpha=0.8,
                    label=f'Random Classifier (AUC = {pos_rate:.3f})')
    
    # ConfiguraÃ§Ãµes do grÃ¡fico
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall', fontsize=22)
    plt.ylabel('Precision', fontsize=22)
    
    plt.title('Precision-Recall Curves (Binary Classification)', fontsize=24)
    
    plt.legend(loc="lower left", fontsize=18, framealpha=0.9)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path + '_binary.png', dpi=300, bbox_inches='tight')
        plt.savefig(save_path + '_binary.pdf', bbox_inches='tight')
        print(f"Precision-Recall Curve salva em: {save_path}")
        
    # RelatÃ³rio final
    print(f"\nRelatÃ³rio Precision-Recall:")
    print(f"  Modelos plotados: {models_plotted}")
    if models_with_errors:
        print(f"  Modelos com erro: {', '.join(models_with_errors)}")


def generate_all_plots():
    """
    FunÃ§Ã£o principal para gerar todos os grÃ¡ficos usando prediÃ§Ãµes salvas
    Suporta tanto modo default quanto optimized (com ou sem test_predictions)
    """
    print("Gerando grÃ¡ficos de performance dos modelos usando prediÃ§Ãµes salvas...")
    print("="*70)
    
    print("Carregando prediÃ§Ãµes salvas dos modelos...")
    model_results, experiment_dir, detected_mode = load_saved_predictions()
    
    if not model_results:
        print("âŒ Nenhuma prediÃ§Ã£o encontrada!")
        print("Execute primeiro os modelos usando main.py para gerar as prediÃ§Ãµes.")
        return
    
    if not experiment_dir:
        print("âŒ Nenhum diretÃ³rio de experimento encontrado!")
        return
    
    # Filtrar modelos que tÃªm test_predictions (caso haja alguns sem)
    models_with_predictions = {k: v for k, v in model_results.items() if v.get('predictions') is not None}
    
    if not models_with_predictions:
        print("âŒ Nenhum modelo com test_predictions encontrado!")
        if detected_mode == 'optimized':
            print("âš ï¸  Modelos otimizados sem test_predictions nÃ£o podem gerar curvas ROC/PR")
            print("    VocÃª pode re-executar com a versÃ£o atualizada do cÃ³digo que salva test_predictions")
        return
    
    # Detectar tipo de classificaÃ§Ã£o
    first_model = next(iter(models_with_predictions.values()))
    predictions = first_model['predictions']
    y_true = np.array(predictions['y_true'])
    y_pred_proba = np.array(predictions['y_pred_proba'])
    classification_type, n_classes, class_names = detect_classification_type(y_true, y_pred_proba)
    
    # Criar estrutura de diretÃ³rios
    curves_dir = os.path.join(experiment_dir, "curves")
    os.makedirs(curves_dir, exist_ok=True)
    
    models_loaded = len(models_with_predictions)
    models_skipped = len(model_results) - models_loaded
    print(f"âœ… PrediÃ§Ãµes encontradas para {models_loaded} modelos: {list(models_with_predictions.keys())}")
    if models_skipped > 0:
        print(f"âš ï¸  {models_skipped} modelos sem test_predictions foram ignorados")
    print(f"ðŸ“Š Tipo de classificaÃ§Ã£o: {classification_type} ({n_classes} classes)")
    print(f"ðŸ”„ Modo detectado: {detected_mode.upper()}")
    
    if classification_type == 'multiclass' and n_classes > 2:
        print(f"\nðŸ“ˆ AnÃ¡lise multiclasse detectada ({n_classes} classes)")
        generate_multiclass_plots(models_with_predictions, class_names, curves_dir)

    else:
        print(f"\nðŸ“ˆ AnÃ¡lise binÃ¡ria detectada")
        print("\nGerando ROC Curves...")
        roc_save_path = os.path.join(curves_dir, "roc_comparison")
        plot_roc_curve(models_with_predictions, save_path=roc_save_path)
        
        print("\nGerando PR Curves...")
        pr_save_path = os.path.join(curves_dir, "pr_comparison")
        plot_precision_recall_curve(models_with_predictions, save_path=pr_save_path)
    
    print(f"\nðŸŽ‰ Todos os grÃ¡ficos gerados com sucesso!")

if __name__ == "__main__":
    print("Executando geraÃ§Ã£o de grÃ¡ficos...")
    generate_all_plots()
